---
manual_name: /smartspec_ui_validation Manual (EN)
manual_version: 5.6
compatible_workflow: /smartspec_ui_validation
compatible_workflow_versions: 5.6.2 – 5.6.x
role: user/operator manual (frontend, QA, UX, release, platform)
---

# /smartspec_ui_validation Manual (v5.6, English)

## 1. Overview

This manual explains how to use the workflow:

> `/smartspec_ui_validation v5.6.x` (e.g., 5.6.2, 5.6.3)

The workflow is a **governance layer for UI correctness & validation**.
It focuses on:

- checking that implemented UI **matches specs / UI JSON / UX flows**
- assessing how well UI has been **validated** across:
  - behavior and flows (happy paths + edge cases)
  - error states and input validation
  - accessibility (a11y)
  - visual regression / snapshot testing
  - internationalization (i18n) / localization
  - cross-environment / cross-device combinations
- producing a **UI validation report** per UI unit (route/screen/flow)
  with status fields and `blocking_for_release` signals
- from v5.6.3 onwards, also reading **AI-generated `ui.json` metadata**
  (origin, review status, design system version, style preset) to
  understand how much you can trust each UI spec

> **Important:**
> - The workflow does **not** run tests or launch browsers/devices.
> - It does **not** modify code, configs, or test files.
> - It only reads existing artifacts (specs, UI JSON, test reports) and
>   synthesizes a governance view.

### 1.1 Why you need this workflow

Without a structured UI validation governance layer, teams often face:

- critical flows that are assumed to be tested but lack any real
  coverage
- missing tests for edge cases and error/validation states
- a11y reports existing somewhere in CI, but no clear connection to
  specific flows/screens
- visual regression tests with unclear mapping to business-critical
  screens
- i18n gaps where important locales are not adequately validated
- release boards seeing only a "tests passed" percentage in CI, with no
  mapping back to UI specs or flows
- in systems where `ui.json` is generated by AI by default:
  - some flows use AI-written specs that no human has ever reviewed
  - you have no structured way to see which flows rely most on
    unreviewed AI specs

`/smartspec_ui_validation` connects:

> **spec / UI JSON (including AI-generated) ↔ implementation ↔ test reports**

into a single report that answers:

> "Before this release, how well have our critical UI flows been
> validated, and where are the gaps?"  
> and  
> "Where are we relying on unreviewed AI-generated `ui.json` for flows
> that really matter?"

### 1.2 Benefits of using it

- Provides a **shared language** for frontend, QA, UX, and release
  stakeholders.
- Shows **validation status per UI unit** in a structured way.
- Makes it clear, per flow, whether:
  - the spec/UI JSON comes from AI vs humans
  - the spec has been reviewed
  - the `ui.json` structure looks strong or weak
- Helps release boards decide:
  - which flows are ready
  - which flows need additional tests
  - where risk is accepted with clear rationale
- Produces an **audit artifact** that shows UI validation was reviewed
  before release.

### 1.3 Risks if you don’t use it (or equivalent governance)

- Releasing critical flows (login/checkout/consent) with no real test
  coverage.
- Shipping blocking accessibility issues unnoticed.
- i18n bugs discovered late in production because locales were not
  validated systematically.
- Relying on AI-generated `ui.json` as the source of truth without any
  visibility into which flows are still unreviewed.
- No clear artifact that shows which screens/flows passed validation,
  making management/auditor conversations harder.

---

## 2. What’s New in v5.6

### 2.1 Per-UI-unit status model

Each UI unit now has a consistent status model including:

- `criticality = CRITICAL | HIGH | MEDIUM | LOW | UNKNOWN`
- `spec_coverage_status = FULL | PARTIAL | NONE | UNKNOWN`
- `behavior_validation_status = STRONG | BASIC | NONE | UNKNOWN`
- `error_state_coverage_status = COMPREHENSIVE | PARTIAL | NONE | UNKNOWN`
- `input_validation_coverage_status = COMPREHENSIVE | PARTIAL | NONE | UNKNOWN`
- `accessibility_status = GOOD | ISSUES_NON_BLOCKING | ISSUES_BLOCKING | UNKNOWN`
- `visual_regression_status = COVERED | PARTIAL | NONE | UNKNOWN`
- `i18n_status = COVERED | PARTIAL | NONE | UNKNOWN`
- `cross_env_status = COVERED | PARTIAL | NONE | UNKNOWN`
- `risk_level = LOW | MEDIUM | HIGH | CRITICAL`
- `blocking_for_release = true | false`

### 2.2 Clear strict-mode rules

In `--safety-mode=strict`:

- for units with `criticality in {CRITICAL, HIGH}`:
  - missing/unknown behavior validation or spec coverage → blocking
  - `accessibility_status=ISSUES_BLOCKING` → blocking
  - no/unknown visual regression coverage for visually critical units →
    usually blocking
  - no/unknown i18n coverage for required locales → usually blocking
  - partial/none cross-environment coverage where multiple envs/devices
    are required → affects risk and may be blocking

### 2.3 Criticality derivation

Criticality is derived from:

- registries (e.g., `.spec/registry/*` that mark critical flows)
- tags in specs / UI JSON (e.g., `critical: true`, `importance: "high"`)
- explicit overrides via `--ui-critical-targets`

If nothing is defined, units default to `MEDIUM` or `LOW` with a note
that criticality is uncertain.

### 2.4 Relationship with release readiness

- The report produces `blocking_for_release` as an **additional
  governance signal** for `/smartspec_release_readiness` and other
  workflows.
- It does not override other workflows, but `blocking` items should be
  explicitly discussed and resolved (or formally risk-accepted) by the
  release decision-makers.

### 2.5 AI-generated `ui.json` signals (added in v5.6.3)

From v5.6.3 onwards, each UI unit also includes signals about
AI-generated `ui.json` quality and trustworthiness:

- `ui_spec_origin = AI | HUMAN | MIXED | UNKNOWN`
  - indicates whether the spec/UI JSON is primarily AI-generated,
    human-authored, or mixed.
- `ui_spec_review_status = UNREVIEWED | DESIGNER_APPROVED | OVERRIDDEN | UNKNOWN`
  - derived from `meta.review_status` or registries; shows whether the
    spec has been reviewed.
- `ui_json_quality_status = STRONG | OK | WEAK | BROKEN | UNKNOWN`
  - summarizes how coherent and complete the `ui.json` is relative to
    the design system and flows.

A new flag is introduced:

- `--ui-json-ai-strict`
  - enables stricter rules for AI-generated UI JSON.
  - when a flow is critical/high and `ui_spec_origin=AI` with
    `ui_spec_review_status=UNREVIEWED` or `ui_json_quality_status` is
    low, risk is raised and may become `blocking_for_release` in strict
    mode.

---

## 3. Backward Compatibility Notes

- This v5.6 manual targets `/smartspec_ui_validation` from
  **v5.6.2 onwards** (5.6.x series).
- `--strict` remains an alias for `--safety-mode=strict`.
- v5.6.3 adds `--ui-json-ai-strict` and AI-related signals
  (`ui_spec_origin`, `ui_spec_review_status`, `ui_json_quality_status`)
  in an additive way.

---

## 4. Core Concepts

### 4.1 What is a UI unit?

A UI unit is typically one of:

- a route (e.g., `/checkout`, `/login`)
- a screen/page (e.g., "Profile Screen")
- a user flow (e.g., "Checkout Flow", "Password Reset Flow")

Each unit receives a status record using the fields listed above.

### 4.2 Criticality

Criticality reflects how important a UI unit is, e.g.:

- CRITICAL
  - login, checkout, payment, consent, account recovery
  - flows with regulatory or strong legal impact
- HIGH
  - flows with strong conversion or business impact
- MEDIUM / LOW
  - less critical but still relevant flows

Criticality comes from:

- registries (critical flows, SLOs)
- spec / UI JSON tags
- explicit `--ui-critical-targets`

### 4.3 Signals from AI-generated `ui.json`

For systems where AI generates `ui.json` by default, each UI unit also
has:

- `ui_spec_origin` → source of the spec (AI vs human vs mixed)
- `ui_spec_review_status` → whether the spec was reviewed
- `ui_json_quality_status` → structural quality of `ui.json`

These signals are combined with other statuses to compute `risk_level`
and `blocking_for_release`, especially in strict mode and when
`--ui-json-ai-strict` is enabled.

### 4.4 Governance-only scope

This workflow does **not**:

- run cypress/playwright/jest or similar tools
- create or modify test files
- output ready-to-run test commands as implicitly "approved"

It only reads specs/UI JSON + test reports and produces a governance
summary.

---

## 5. Quick Start Examples

### 5.1 Validate checkout UI (web)

```bash
smartspec_ui_validation \
  --spec-ids=checkout_service \
  --run-label=checkout-ui-pre-release \
  --ui-spec-paths=".spec/ui/**/*.json" \
  --ui-test-report-paths=".reports/ui/e2e/**/*.json" \
  --ui-snapshot-report-paths=".reports/ui/snapshot/**/*.json" \
  --ui-accessibility-report-paths=".reports/ui/a11y/**/*.json" \
  --ui-i18n-report-paths=".reports/ui/i18n/**/*.json" \
  --target-envs=web \
  --target-browsers=chromium,firefox \
  --report-format=md \
  --stdout-summary
```

### 5.2 Strict mode for login + consent flows with strict AI UI JSON

```bash
smartspec_ui_validation \
  --spec-ids=identity_service \
  --run-label=login-consent-ui-strict \
  --ui-critical-targets=login,consent_flow \
  --ui-spec-paths=".spec/ui/**/*.json" \
  --ui-test-report-paths=".reports/ui/e2e/**/*.json" \
  --ui-accessibility-report-paths=".reports/ui/a11y/**/*.json" \
  --target-envs=web \
  --target-browsers=chromium,firefox,safari \
  --safety-mode=strict \
  --ui-json-ai-strict \
  --report-format=json \
  --stdout-summary
```

### 5.3 Inline UI project (no UI JSON)

```bash
smartspec_ui_validation \
  --spec-ids=legacy_portal \
  --run-label=legacy-portal-ui-validation \
  --ui-json-mode=disabled \
  --ui-spec-paths="specs/ui/**/*.md" \
  --ui-test-report-paths=".reports/ui/e2e/**/*.json" \
  --report-format=md
```

---

## 6. CLI / Flags Cheat Sheet

- Scope & label
  - `--spec-ids`
  - `--ui-targets`
  - `--ui-critical-targets`
  - `--include-dependencies`
  - `--run-label`
- UI spec & implementation
  - `--ui-spec-paths`
  - `--ui-impl-paths`
  - `--ui-json-mode=auto|required|disabled`
  - `--ui-json-ai-strict` (tighten rules for AI-generated `ui.json`)
- Test & report artifacts
  - `--ui-test-report-paths`
  - `--ui-snapshot-report-paths`
  - `--ui-accessibility-report-paths`
  - `--ui-i18n-report-paths`
- Environment & platform
  - `--target-envs`
  - `--target-browsers`
  - `--target-devices`
- Multi-repo & safety
  - `--workspace-roots`
  - `--repos-config`
  - `--registry-dir`, `--registry-roots`
  - `--index`, `--specindex`
  - `--safety-mode=normal|strict` (`--strict`)
- Output & KiloCode
  - `--report-format=md|json`
  - `--report-dir`
  - `--stdout-summary`
  - `--kilocode`, `--nosubtasks`

---

## 7. Reading the UI Validation Report

Typical structure:

1. **Scope overview**
   - spec-ids, environments, browsers, devices
   - date, run-label

2. **Per-UI-unit table**
   - for each `unit_id`:
     - `criticality`
     - `ui_spec_origin`
     - `ui_spec_review_status`
     - `ui_json_quality_status`
     - `spec_coverage_status`
     - `behavior_validation_status`
     - `error_state_coverage_status`
     - `input_validation_coverage_status`
     - `accessibility_status`
     - `visual_regression_status`
     - `i18n_status`
     - `cross_env_status`
     - `risk_level`
     - `blocking_for_release`
     - `notes`

3. **Gaps & risks**
   - highlighted units with `risk_level=HIGH/CRITICAL` or
     `blocking_for_release=true`
   - special attention to units where:
     - `ui_spec_origin=AI` and `ui_spec_review_status=UNREVIEWED`
     - `ui_json_quality_status=WEAK/BROKEN`

4. **Summary**
   - counts by risk level
   - counts by blocking vs non-blocking

> Note: When `--report-format=json`, JSON is the canonical structure.
> The markdown report should mirror the same fields to support tooling.

---

## 8. KiloCode Usage Examples

### 8.1 Kilo for web + mobile apps

```bash
smartspec_ui_validation \
  --spec-ids=mobile_app,web_portal \
  --include-dependencies \
  --run-label=web-mobile-ui-validation \
  --ui-spec-paths=".spec/ui/**/*.json" \
  --ui-test-report-paths=".reports/ui/e2e/**/*.json" \
  --ui-accessibility-report-paths=".reports/ui/a11y/**/*.json" \
  --target-envs=web,ios,android \
  --kilocode \
  --stdout-summary
```

On Kilo:

- Orchestrator decomposes work by spec-id/flow group.
- Code mode reads reports read-only.
- Results are aggregated per-unit in the final report.

### 8.2 Disabling subtasks for a small scope

```bash
smartspec_ui_validation \
  --spec-ids=small_widget_service \
  --run-label=small-widget-ui-validation \
  --ui-spec-paths=".spec/ui/small_widget/*.json" \
  --ui-test-report-paths=".reports/ui/e2e/small_widget/**/*.json" \
  --kilocode \
  --nosubtasks
```

---

## 9. Multi-repo / Multi-registry Examples

### 9.1 Monorepo with multiple frontend apps

```bash
smartspec_ui_validation \
  --spec-ids=web_portal,admin_portal \
  --run-label=portal-ui-validation \
  --repos-config=.spec/repos.yaml \
  --registry-dir=.spec/registry \
  --ui-spec-paths=".spec/ui/**/*.json" \
  --ui-test-report-paths=".reports/ui/e2e/**/*.json" \
  --report-format=md
```

### 9.2 Multi-repo, multi-team

```bash
smartspec_ui_validation \
  --spec-ids=teamA_web,teamB_mobile \
  --run-label=web-mobile-crossflow-validation \
  --workspace-roots="../teamA;../teamB" \
  --registry-dir=../platform/.spec/registry \
  --ui-spec-paths="../teamA/.spec/ui/**/*.json;../teamB/.spec/ui/**/*.json" \
  --ui-test-report-paths="../teamA/.reports/ui/e2e/**/*.json;../teamB/.reports/ui/e2e/**/*.json"
```

---

## 10. UI JSON vs Inline UI

### 10.1 JSON-first UI

- Screens/flows are defined in files such as:
  - `.spec/ui/<app>.ui.json` or similar.
- The workflow treats UI JSON as the **primary source of truth**.
- When `--ui-json-mode=required` and UI JSON is missing:
  - this is treated as a validation gap for
    `spec_coverage_status` and `ui_json_quality_status`.
- For AI-generated UI JSON it is strongly recommended to always
  populate meta fields:
  - `source`, `generator`, `design_system_version`, `style_preset`,
    `review_status`.

### 10.2 Inline UI / opt-out

- No separate UI JSON; specs are in markdown/docs.
- Use `--ui-json-mode=disabled`.
- The workflow still builds a report from specs + test reports.

---

## 11. Benefits vs Risks of Not Using

### 11.1 Benefits of using

- Clear visibility into how thoroughly UI has been validated per
  flow/screen.
- Clarity about how much each flow depends on AI-generated `ui.json`,
  and whether it has been reviewed.
- Better release meeting discussions: not just "tests passed X%" but
  "these critical flows are covered at level Y with AI UI JSON quality
  Z".
- Easier prioritization of work on tests, a11y, and i18n.

### 11.2 Risks if you don’t use it

- Shipping critical flows with poor or unknown validation coverage.
- Shipping flows where `ui.json` is AI-generated and still
  `UNREVIEWED`.
- Building silent UX/a11y/i18n technical debt.
- Lacking evidence that UI correctness was reviewed before releases.

---

## 12. FAQ / Troubleshooting

**Q1: What if we have almost no UI tests or reports?**  
The workflow can still run but will mark many statuses as
`NONE`/`UNKNOWN`. That is a strong signal to invest in UI tests before
major releases, especially for critical flows.

**Q2: Will the workflow fix code or generate tests for us?**  
No. It only indicates where validation gaps are. You can use other
prompt/workflow patterns for test generation if desired.

**Q3: Should we always use strict mode?**  
Not always. Use `strict` for flows with high `criticality` (login,
checkout, payments, consent, identity, etc.), especially when
`ui.json` is AI-generated and still `UNREVIEWED`.

---

End of `/smartspec_ui_validation v5.6.x` manual (English).
If future versions significantly change the status model or strict-mode
semantics, issue a new manual (e.g., v5.7) and clearly document the
compatible workflow versions.

